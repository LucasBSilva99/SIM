{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['# Text Analysis for Topic Extraction Pipeline\\n',\n",
       "    '\\n',\n",
       "    'In this module we focus on the most widespread and present media form: **text**.  \\n',\n",
       "    'Text is every-where, in spoken language, books, websites, and so on.\\n',\n",
       "    '\\n',\n",
       "    'It is no surprise that the success of big search engines is based on the appropriate analysis of text. \\n',\n",
       "    'Deriving ‘meaning’ from text is far from trivial, indeed it is a very difficult task.\\n',\n",
       "    'Consider that meaning in text is created by distributions of words in specific language, following very specific and diverse grammar rules.\\n',\n",
       "    'These arrangements are further nuanced by cultural codes, shorthands, metaphors, analogies, irony, specific references and so on.,\\n',\n",
       "    '\\n',\n",
       "    'So, in this module, we are going to analyse this kind of media to extract topics in with the help of some statistical tools that can be seen as semi-automated. \\n',\n",
       "    'Indeed, it is only semi-automated due to the fact that we are going to create a pipeline (a sequence of steps) that is going allow us to analyse some statistics about the corpus and derive some meaning - so, the end result will always depend on human interpretation.\\n',\n",
       "    '\\n',\n",
       "    'In order to do so, we are going to use a family of techniques known as **Bag of Words**.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 1. A primer to the Bag Of Words Pipeline\\n',\n",
       "    '\\n',\n",
       "    'The life cycle of statistical text processing can be seen as a four stages core, that are ilustrated in the figure below. \\n',\n",
       "    '\\n',\n",
       "    'The starting point of this process is the availability of a **corpus** - which is a collection of text documents.\\n',\n",
       "    'For example, we can have a corpus on political debates, cooking recipes, the news broadcast by a given agency in a time period, and so on. \\n',\n",
       "    '\\n',\n",
       "    'Corpora (plural ofcorpus) thus often gather many documents on a given theme, and our goal is to find the different topics that make up that theme.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['![Bag Of Words](./imgs/bag_of_words_pipeline.png)\\n',\n",
       "    'Figure 1. Bag Of Words Pipeline']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 1.1 It all starts with a corpus\\n',\n",
       "    '\\n',\n",
       "    'Any kind of text analysis always starts with a corpus. Maybe that corpus are Tweeks, or maybe that corpus are News.\\n',\n",
       "    'And that corpus can be in many data formats - Web Pages that need to be scrapped, Text Files that need to be parsed, or some binary format that needs to be interpreted.\\n',\n",
       "    '\\n',\n",
       "    \"Let's make some examples with a text file that contains several news for a given time period, and let's parse the file!\\n\",\n",
       "    '\\n',\n",
       "    'Why do we need to parse the file? Well, if we want to understand the words of a given document in the file, we need to be able to access to different documents as we need - If I want to access to document 17, I must have a way to do.\\n',\n",
       "    'Therefore, we need to create some kind of structure in memory to process these documents on the file!']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 1.2 Step 1 - Parse the Data\\n',\n",
       "    '\\n',\n",
       "    'To parse the data into memory, we can start by getting every document in the file as a string, and store each document into a List.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 21,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['document_list = []\\n',\n",
       "    '# this is the dataset that we have been working with in the classes\\n',\n",
       "    '# change the path of the file to where it is in your computer\\n',\n",
       "    'with open(\\'data/NYT_Corpus.txt\\', encoding = \"UTF-8\") as f:   \\n',\n",
       "    '    \\n',\n",
       "    '    for line in f:\\n',\n",
       "    '        found_url = line[:9] == \"URL: http\" and line[-6:]==\".html\\\\n\"\\n',\n",
       "    '        if found_url:\\n',\n",
       "    '            f.readline()\\n',\n",
       "    '            document_list.append(\"\")\\n',\n",
       "    '        else:\\n',\n",
       "    '            document_list[len(document_list)-1] += line\\n',\n",
       "    '\\n',\n",
       "    'corpus = [doc for doc in document_list if len(doc) > 0]']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['And now we have some data in memory with which we can start the pipeline described above!']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 2. Clean Documents\\n',\n",
       "    '\\n',\n",
       "    'Now that we have our corpus, the first stage is concerned with cleaning the documents.The main idea here is to remove ‘noise’ in the form of text that has little to say about what a documentis speaking about.  \\n',\n",
       "    '\\n',\n",
       "    'Statistical text analysis (STA) is concerned with the distributions of words. This is so in the sense that a document that contains e.g. many copies of the words ‘oven’, ‘cook’and ‘onion’ are likely to be about recipes/food.  \\n',\n",
       "    'STA is not concerned with grammar rules of anykind.  For this reason, all connectors, punctuation marks and so on are not important to STA(and indeed they are eliminated as we will see later).  \\n',\n",
       "    '\\n',\n",
       "    'Other methods for analysing text in thefield of Deep Natural Language processing are interested in (and use) grammar rules but we donnot study Deep NLP in this course.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### What is our goal in this step\\n',\n",
       "    '\\n',\n",
       "    'Now we have a lot of words in the documents, as well as some other chars (e.g. punctuation). At the end of this Data Cleaning step, we need to have:\\n',\n",
       "    '\\n',\n",
       "    '1. No punctuation chars\\n',\n",
       "    '2. \"normalized\" words in two senses:\\n',\n",
       "    '    - we and all the words in the same case (lower case)\\n',\n",
       "    '    - in the sense that mapping, mapper and map are all words from the same family, and for this analysis we want these to be the same word instead of different words\\n',\n",
       "    '3. Clean words that are not going to help us understand topics']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 2.1 Interpreting tokens in each document\\n',\n",
       "    '\\n',\n",
       "    'So, one first step that we need to model is to understand what we are going to define as a valid tokens. For every document, as we are going thought it, we are going to need to filter valid tokens - only letting pass tokens that we have defined as valid.\\n',\n",
       "    '\\n',\n",
       "    \"In this exercise, let's define a **valid token** as any sequence of letters (being if upper or lowercase) as well as any sequence of numbers.\\n\",\n",
       "    '\\n',\n",
       "    \"In order to do so, let's use a `regex` pattern which is `\\\\w`, meaning that:\\n\",\n",
       "    '1. We want a sequence with at least of letter, being it upper or lower case\\n',\n",
       "    '2. We want any sequence with at least one number\\n',\n",
       "    '3. The catch on using this pattern is that we are going to allow the underscore char as well (`_`), but this allow our regex pattern to be a small and readable string :)']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 2.2 Putting the words in lower case\\n',\n",
       "    '\\n',\n",
       "    'This step can be done either before the tokenization of after the tokenization, given that we are creating tokens out of both lower and upper case chars.\\n',\n",
       "    'So, this step is here as the second not because it needs to be done here, but for us not to forget that this needs to be done - being it before, during or after the tokenization']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 2.3 Stemming the words\\n',\n",
       "    '\\n',\n",
       "    'One of our goals is this step is to normalize words of the same family.\\n',\n",
       "    '\\n',\n",
       "    'For example, if I have the words mapping, mapper and map, I want these different words that are the same semantic to be the same word.\\n',\n",
       "    '\\n',\n",
       "    '\\n',\n",
       "    'A process that can help us do that is called Stem.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 22,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['from nltk.tokenize import RegexpTokenizer\\n',\n",
       "    'from nltk.stem import PorterStemmer\\n',\n",
       "    '\\n',\n",
       "    \"tokenizer = RegexpTokenizer(r'\\\\w+')\\n\",\n",
       "    'ps = PorterStemmer()\\n',\n",
       "    '\\n',\n",
       "    'def mytokeniser(s):\\n',\n",
       "    '    aux = [w.lower() for w in tokenizer.tokenize(s)]\\n',\n",
       "    '    return list(map(ps.stem, aux))\\n',\n",
       "    '\\n',\n",
       "    'tokenised_corpus = list(map(mytokeniser, corpus))\\n']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### Cleaning words that will not help us understand topics\\n',\n",
       "    '\\n',\n",
       "    \"So, given that Statistical Text Analysis (STA) is about data distribution, events that are constants (e.g. words that appear in every document - stop words for example) and events that are rare (e.g. word that appear in, for example, only in 3% in the documents of the corpus) are not going to be relevant for ou topic analysis, aren't they? \\n\",\n",
       "    '\\n',\n",
       "    'Thinkg about this a little bit, and let this sink in.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['Does it makes sense now? Awesome!\\n',\n",
       "    '\\n',\n",
       "    'So, we now need a way to understand which words are which.\\n',\n",
       "    '\\n',\n",
       "    'The way we are going to do this is:\\n',\n",
       "    '1. Creating a vocabulary - a set of all the terms in the corpus\\n',\n",
       "    '2. Score all the terms with a metric that help us understand the incident of a given term in a corpus.\\n',\n",
       "    '    - This score will put the rare events in one extreme, and the constant events the another extreme (e.g. rare terms having the higher values, and constant terms having the lower values).\\n',\n",
       "    '    - The above line yields that, for topic analysis, we want the words that are in between of those two\\n',\n",
       "    \"3. Based on these metrics, let's filter the terms and stay only with those relevant for topic analysis \"]},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 2.4 Creating a vocabulary']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['Given that this corpus has a lot of document, I am going to sample the corpus and will only use 100 documents']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 24,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'name': 'stdout',\n",
       "     'output_type': 'stream',\n",
       "     'text': ['My vocabolary size is 7642\\n']}],\n",
       "   'source': ['tokenized_corpus_sampled = tokenised_corpus[:100]\\n',\n",
       "    '\\n',\n",
       "    'vocab = set()\\n',\n",
       "    '\\n',\n",
       "    'for doc in tokenized_corpus_sampled:\\n',\n",
       "    '    vocab = vocab.union(set(doc))  \\n',\n",
       "    '\\n',\n",
       "    'print(f\"My vocabolary size is {len(vocab)}\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['### 2.5 Scoring extreme terms\\n',\n",
       "    '\\n',\n",
       "    'The metric we are going to use to score the terms and therefore to get the extreme event is going to be the *IDF* metric.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['#### The IDF Metric\\n',\n",
       "    '\\n',\n",
       "    'Suppose  for  example  that  in  our  corpus  the  word  ‘rice’  appears  in  every  document.   \\n',\n",
       "    'Suppose that you are the librarian keeping this corpus,  and somebody comes searching for a subset of documents in a given topic from your corpus.  \\n',\n",
       "    'Imagine this library visitor tells you ‘rice’.  You go into the box to fetch all the documents that contain that word.  Clearly you will come backwith the entire box because every document in it contains that term.  \\n',\n",
       "    'Was ‘rice’ a helpful term to support the library visitor’s needs?  Not really.  In fact not helpful at all. \\n',\n",
       "    '\\n',\n",
       "    'This is where Inverse Document Frequency or IDF comes in handy.  This number will represent the importance of a term in a given corpus, calculated using the following formula:\\n',\n",
       "    '\\n',\n",
       "    '$IDF = log(\\\\frac{N}{df_t})$\\n',\n",
       "    '\\n',\n",
       "    'Where:\\n',\n",
       "    '- N is the number of document in the corpus\\n',\n",
       "    '- $df_t$ is the number of documents where the term $t$ is in']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 26,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['def idf(term, corpus):\\n',\n",
       "    '    cnt =  sum([1 if term in doc else 0 for doc in corpus])\\n',\n",
       "    '    return math.log10( len(corpus) / cnt )\\n',\n",
       "    '\\n',\n",
       "    'idfvocab = {}\\n',\n",
       "    '\\n',\n",
       "    'for term in vocab:\\n',\n",
       "    '    term_idf = idf(term, tokenized_corpus_sampled)\\n',\n",
       "    '    idfvocab[term] = term_idf\\n']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': [\"### 2.6 Let's filter out those extreme events\\n\",\n",
       "    '\\n',\n",
       "    'So, we now have our terms with a given metric scored. \\n',\n",
       "    'For us to filter the extreme events, there are several ways for doing so. \\n',\n",
       "    '\\n',\n",
       "    'We can think that we want to remove, for example, the upper 25% of the tokens and the lower 25% of the tokens.\\n',\n",
       "    '\\n',\n",
       "    'We can also say that we want, at most only 200 tokens that are somewhere in the middle of the distribution to pass this filter.\\n',\n",
       "    '\\n',\n",
       "    'The following example will be with the latter mindset.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 27,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'name': 'stdout',\n",
       "     'output_type': 'stream',\n",
       "     'text': ['Min is 0.0 and max is 2.0\\n']}],\n",
       "   'source': ['idfvocab_it = [(el[0],el[1]) for el in idfvocab.items()]\\n',\n",
       "    '\\n',\n",
       "    'aux = np.array( idfvocab_it )\\n',\n",
       "    'low = float( min( aux[:,1] ) )\\n',\n",
       "    'high = float( max( aux[:,1] ) )\\n',\n",
       "    '\\n',\n",
       "    'print(f\"Min is {low} and max is {high}\")']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 29,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'data': {'text/plain': ['150']},\n",
       "     'execution_count': 29,\n",
       "     'metadata': {},\n",
       "     'output_type': 'execute_result'}],\n",
       "   'source': ['def keep_terms( lower, upper, threshold, step, idf_vocabulary ):\\n',\n",
       "    '    low = lower\\n',\n",
       "    '    up = upper\\n',\n",
       "    '    candidates = idf_vocabulary\\n',\n",
       "    '    while len(candidates) > threshold:\\n',\n",
       "    '        #print(f\"current vocabolary size is {len(candidates)}\")\\n',\n",
       "    '        low = low + step\\n',\n",
       "    '        up = up - step\\n',\n",
       "    '        candidates = [  term for term in idf_vocabulary if term[1] >= low and term[1] <= up  ]\\n',\n",
       "    '    return candidates\\n',\n",
       "    '\\n',\n",
       "    '\\n',\n",
       "    'cnd = keep_terms(low, high, 200, 0.005, idfvocab_it)\\n',\n",
       "    'len(cnd)']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['By the end of this process, we now have our Bag of Words to continue this analysis! Notice that every document is going to be encoded with this bag of words in a vector space - we now have a unified (single) corpus representation!']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Construction of the TF-IDF Matrix\\n',\n",
       "    '\\n',\n",
       "    'This step is the core of the STA life cycle. What  is  interesting  here  is  that,  while  starting with a corpus made of disjoint elements (a collection of documents),  we end up with a single corpus representation.  \\n',\n",
       "    '\\n',\n",
       "    'This single representation unifies the information we have about the comprised documents.  Therefore we can use this single data structure to reason about the entire corpus!  \\n',\n",
       "    'For this goal, working with an universal dictionary of terms is essential.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['#### The TF Metric\\n',\n",
       "    '\\n',\n",
       "    'Term Frequency or simply TF is a numeric quantity used to express the importance of a term inside a document.  \\n',\n",
       "    'In its raw form, it is simply the count of times a term appears in a document.\\n',\n",
       "    '\\n',\n",
       "    'However, here we compute TF as a proportion, by dividing this count by the total number of tokens  in  the  (stemmed)  document.   ']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 30,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['vc = np.array(cnd) #a matrix, with column 0 being terms and column 1 being idf\\n',\n",
       "    'vc_terms = vc[:,0] \\n',\n",
       "    '\\n',\n",
       "    'def normTFx(term,doc):\\n',\n",
       "    '    return doc.count(term)/len(doc)']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['#### The TF-IDF Metric\\n',\n",
       "    '\\n',\n",
       "    'The final quantity that we will use to measure the importance of a term inside a document that belongs to a corpus os the standard TF.IDF measure which is simply:\\n',\n",
       "    '\\n',\n",
       "    '$TF.IDF^d_t = TF^d_t * IDF_t$\\n',\n",
       "    '\\n',\n",
       "    'Here $t$ refers as always to the term, and $d$ to a specific document. \\n',\n",
       "    '\\n',\n",
       "    'What is the effect of multiplying the original normalised TF by the IDF? The IDF acts as a modulator.  If the TF is high but the term is everywhere in the corpus, the IDF will be low, so the TF is brought down.  If a TF is medium, but the IDF is high then its importance is modulated upwards.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 33,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['def tfidfmat(corpus, tl) :\\n',\n",
       "    '    mat =[]\\n',\n",
       "    '    for i in tl :\\n',\n",
       "    '        idft = idf(i,corpus)\\n',\n",
       "    '        row = []\\n',\n",
       "    '        for d in corpus :\\n',\n",
       "    '            tft = normTFx(i,d)\\n',\n",
       "    '            tf_idf_term_document = tft*idft\\n',\n",
       "    '            row.append(tf_idf_term_document)\\n',\n",
       "    '        mat.append(row)\\n',\n",
       "    '    return mat    \\n',\n",
       "    '            \\n',\n",
       "    '    \\n',\n",
       "    '\\n',\n",
       "    'tfidf_matrix = tfidfmat(tokenized_corpus_sampled, vc_terms) \\n',\n",
       "    'tfidf_matrix_np = np.array(tfidf_matrix)']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 34,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'data': {'text/plain': ['(150, 100)']},\n",
       "     'execution_count': 34,\n",
       "     'metadata': {},\n",
       "     'output_type': 'execute_result'}],\n",
       "   'source': ['tfidf_matrix_np.shape']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 35,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'name': 'stdout',\n",
       "     'output_type': 'stream',\n",
       "     'text': ['The document with index 0 contains 833 words\\n',\n",
       "      'The term with index 0 is `parent`\\n',\n",
       "      'The importance of the term `parent` in the document with idx = 0 is 0.0\\n']}],\n",
       "   'source': ['print(f\"The document with index 0 contains {len(tokenized_corpus_sampled[0])} words\")\\n',\n",
       "    'print(f\"The term with index 0 is `{vc_terms[0]}`\")\\n',\n",
       "    '\\n',\n",
       "    'print(f\"The importance of the term `{vc_terms[0]}` in the document with idx = 0 is {tfidf_matrix_np[0,0]}\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['Now  we  have  all  we  need  to  construct  our  single representation  of  the  corpus  as  a  matrix.   \\n',\n",
       "    '\\n',\n",
       "    'This  matrix has  rows representing  the  terms  of the  universal  dictionary  for  the  corpus,  and  columns  representing  the  contained  documents. Therefore a given cell $S_{t,d}$ of the matrix will contain the corresponding $TF.IDF^d_t$.']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Factorizing the Matrix\\n']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 36,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['from sklearn.decomposition import NMF\\n',\n",
       "    \"model = NMF(n_components=5, init='random', random_state=0)\\n\",\n",
       "    'W = model.fit_transform(tfidf_matrix_np) # loadings\\n',\n",
       "    'H = model.components_ #scores']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 37,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'data': {'text/plain': ['(150, 5)']},\n",
       "     'execution_count': 37,\n",
       "     'metadata': {},\n",
       "     'output_type': 'execute_result'}],\n",
       "   'source': ['W.shape']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 38,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'data': {'text/plain': ['(5, 100)']},\n",
       "     'execution_count': 38,\n",
       "     'metadata': {},\n",
       "     'output_type': 'execute_result'}],\n",
       "   'source': ['H.shape']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 46,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['def get_top_N_terms(matrix_slice, N):\\n',\n",
       "    '    return matrix_slice.argsort()[-N:]']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 55,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['def get_terms_from_slice(loadings_matrix, idx, topN, bag_of_words, orientation=\"col\"):\\n',\n",
       "    \"    '''\\n\",\n",
       "    '        the parameter `orientation` can either be \"col\" or \"row\", so we can process a loadings matrix being it transposed or not\\n',\n",
       "    \"    '''\\n\",\n",
       "    '    k = None\\n',\n",
       "    '    if orientation == \"col\":\\n',\n",
       "    '        k = loadings_matrix[:,idx]\\n',\n",
       "    '    elif orientation == \"row\":\\n',\n",
       "    '        k = loadings_matrix[idx,:]\\n',\n",
       "    '    else:\\n',\n",
       "    '        raise Exception(\"Orientation not recognized\")\\n',\n",
       "    '    k_top5terms_idx = get_top_N_terms(k,topN)\\n',\n",
       "    '    return bag_of_words[k_top5terms_idx]\\n',\n",
       "    '    ']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 60,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'name': 'stdout',\n",
       "     'output_type': 'stream',\n",
       "     'text': [\"The terms with more weight in the component 0 are: ['onlin' 'growth' 'video' 'experi' 'audienc' 'educ' 'publish']\\n\",\n",
       "      \"The terms with more weight in the component 1 are: ['threaten' 'citizen' 'senior' 'educ' 'health' 'crisi' 'senat']\\n\",\n",
       "      \"The terms with more weight in the component 2 are: ['investor' 'air' 'rate' 'educ' 'negoti' 'pay' 'attend']\\n\",\n",
       "      \"The terms with more weight in the component 3 are: ['goal' 'titl' 'franc' 'twitter' 'host' 'central' 'met']\\n\",\n",
       "      \"The terms with more weight in the component 4 are: ['citizen' 'review' 'health' 'refus' 'appeal' 'lawyer' 'polic']\\n\"]}],\n",
       "   'source': ['for k in range(0,W.shape[1]):\\n',\n",
       "    '    # Get terms for the k-th characteristic / topic\\n',\n",
       "    '    print(f\"The terms with more weight in the component {k} are: {get_terms_from_slice(W, k, 7, vc_terms)}\")\\n',\n",
       "    '\\n',\n",
       "    '# here we are printing the top 7, but the this choise is arbitrary - we are going to analyze as much as we need to understand the topics']}],\n",
       " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "   'language': 'python',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "   'file_extension': '.py',\n",
       "   'mimetype': 'text/x-python',\n",
       "   'name': 'python',\n",
       "   'nbconvert_exporter': 'python',\n",
       "   'pygments_lexer': 'ipython3',\n",
       "   'version': '3.7.3'}},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 4}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Text Analysis for Topic Extraction Pipeline\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this module we focus on the most widespread and present media form: **text**.  \\n\",\n",
    "    \"Text is every-where, in spoken language, books, websites, and so on.\\n\",\n",
    "    \"\\n\",\n",
    "    \"It is no surprise that the success of big search engines is based on the appropriate analysis of text. \\n\",\n",
    "    \"Deriving ‘meaning’ from text is far from trivial, indeed it is a very difficult task.\\n\",\n",
    "    \"Consider that meaning in text is created by distributions of words in specific language, following very specific and diverse grammar rules.\\n\",\n",
    "    \"These arrangements are further nuanced by cultural codes, shorthands, metaphors, analogies, irony, specific references and so on.,\\n\",\n",
    "    \"\\n\",\n",
    "    \"So, in this module, we are going to analyse this kind of media to extract topics in with the help of some statistical tools that can be seen as semi-automated. \\n\",\n",
    "    \"Indeed, it is only semi-automated due to the fact that we are going to create a pipeline (a sequence of steps) that is going allow us to analyse some statistics about the corpus and derive some meaning - so, the end result will always depend on human interpretation.\\n\",\n",
    "    \"\\n\",\n",
    "    \"In order to do so, we are going to use a family of techniques known as **Bag of Words**.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. A primer to the Bag Of Words Pipeline\\n\",\n",
    "    \"\\n\",\n",
    "    \"The life cycle of statistical text processing can be seen as a four stages core, that are ilustrated in the figure below. \\n\",\n",
    "    \"\\n\",\n",
    "    \"The starting point of this process is the availability of a **corpus** - which is a collection of text documents.\\n\",\n",
    "    \"For example, we can have a corpus on political debates, cooking recipes, the news broadcast by a given agency in a time period, and so on. \\n\",\n",
    "    \"\\n\",\n",
    "    \"Corpora (plural ofcorpus) thus often gather many documents on a given theme, and our goal is to find the different topics that make up that theme.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"![Bag Of Words](./imgs/bag_of_words_pipeline.png)\\n\",\n",
    "    \"Figure 1. Bag Of Words Pipeline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 1.1 It all starts with a corpus\\n\",\n",
    "    \"\\n\",\n",
    "    \"Any kind of text analysis always starts with a corpus. Maybe that corpus are Tweeks, or maybe that corpus are News.\\n\",\n",
    "    \"And that corpus can be in many data formats - Web Pages that need to be scrapped, Text Files that need to be parsed, or some binary format that needs to be interpreted.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's make some examples with a text file that contains several news for a given time period, and let's parse the file!\\n\",\n",
    "    \"\\n\",\n",
    "    \"Why do we need to parse the file? Well, if we want to understand the words of a given document in the file, we need to be able to access to different documents as we need - If I want to access to document 17, I must have a way to do.\\n\",\n",
    "    \"Therefore, we need to create some kind of structure in memory to process these documents on the file!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 1.2 Step 1 - Parse the Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"To parse the data into memory, we can start by getting every document in the file as a string, and store each document into a List.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 21,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"document_list = []\\n\",\n",
    "    \"# this is the dataset that we have been working with in the classes\\n\",\n",
    "    \"# change the path of the file to where it is in your computer\\n\",\n",
    "    \"with open('data/NYT_Corpus.txt', encoding = \\\"UTF-8\\\") as f:   \\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for line in f:\\n\",\n",
    "    \"        found_url = line[:9] == \\\"URL: http\\\" and line[-6:]==\\\".html\\\\n\\\"\\n\",\n",
    "    \"        if found_url:\\n\",\n",
    "    \"            f.readline()\\n\",\n",
    "    \"            document_list.append(\\\"\\\")\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            document_list[len(document_list)-1] += line\\n\",\n",
    "    \"\\n\",\n",
    "    \"corpus = [doc for doc in document_list if len(doc) > 0]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"And now we have some data in memory with which we can start the pipeline described above!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Clean Documents\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now that we have our corpus, the first stage is concerned with cleaning the documents.The main idea here is to remove ‘noise’ in the form of text that has little to say about what a documentis speaking about.  \\n\",\n",
    "    \"\\n\",\n",
    "    \"Statistical text analysis (STA) is concerned with the distributions of words. This is so in the sense that a document that contains e.g. many copies of the words ‘oven’, ‘cook’and ‘onion’ are likely to be about recipes/food.  \\n\",\n",
    "    \"STA is not concerned with grammar rules of anykind.  For this reason, all connectors, punctuation marks and so on are not important to STA(and indeed they are eliminated as we will see later).  \\n\",\n",
    "    \"\\n\",\n",
    "    \"Other methods for analysing text in thefield of Deep Natural Language processing are interested in (and use) grammar rules but we donnot study Deep NLP in this course.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### What is our goal in this step\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now we have a lot of words in the documents, as well as some other chars (e.g. punctuation). At the end of this Data Cleaning step, we need to have:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. No punctuation chars\\n\",\n",
    "    \"2. \\\"normalized\\\" words in two senses:\\n\",\n",
    "    \"    - we and all the words in the same case (lower case)\\n\",\n",
    "    \"    - in the sense that mapping, mapper and map are all words from the same family, and for this analysis we want these to be the same word instead of different words\\n\",\n",
    "    \"3. Clean words that are not going to help us understand topics\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 2.1 Interpreting tokens in each document\\n\",\n",
    "    \"\\n\",\n",
    "    \"So, one first step that we need to model is to understand what we are going to define as a valid tokens. For every document, as we are going thought it, we are going to need to filter valid tokens - only letting pass tokens that we have defined as valid.\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this exercise, let's define a **valid token** as any sequence of letters (being if upper or lowercase) as well as any sequence of numbers.\\n\",\n",
    "    \"\\n\",\n",
    "    \"In order to do so, let's use a `regex` pattern which is `\\\\w`, meaning that:\\n\",\n",
    "    \"1. We want a sequence with at least of letter, being it upper or lower case\\n\",\n",
    "    \"2. We want any sequence with at least one number\\n\",\n",
    "    \"3. The catch on using this pattern is that we are going to allow the underscore char as well (`_`), but this allow our regex pattern to be a small and readable string :)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 2.2 Putting the words in lower case\\n\",\n",
    "    \"\\n\",\n",
    "    \"This step can be done either before the tokenization of after the tokenization, given that we are creating tokens out of both lower and upper case chars.\\n\",\n",
    "    \"So, this step is here as the second not because it needs to be done here, but for us not to forget that this needs to be done - being it before, during or after the tokenization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 2.3 Stemming the words\\n\",\n",
    "    \"\\n\",\n",
    "    \"One of our goals is this step is to normalize words of the same family.\\n\",\n",
    "    \"\\n\",\n",
    "    \"For example, if I have the words mapping, mapper and map, I want these different words that are the same semantic to be the same word.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"A process that can help us do that is called Stem.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 22,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from nltk.tokenize import RegexpTokenizer\\n\",\n",
    "    \"from nltk.stem import PorterStemmer\\n\",\n",
    "    \"\\n\",\n",
    "    \"tokenizer = RegexpTokenizer(r'\\\\w+')\\n\",\n",
    "    \"ps = PorterStemmer()\\n\",\n",
    "    \"\\n\",\n",
    "    \"def mytokeniser(s):\\n\",\n",
    "    \"    aux = [w.lower() for w in tokenizer.tokenize(s)]\\n\",\n",
    "    \"    return list(map(ps.stem, aux))\\n\",\n",
    "    \"\\n\",\n",
    "    \"tokenised_corpus = list(map(mytokeniser, corpus))\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Cleaning words that will not help us understand topics\\n\",\n",
    "    \"\\n\",\n",
    "    \"So, given that Statistical Text Analysis (STA) is about data distribution, events that are constants (e.g. words that appear in every document - stop words for example) and events that are rare (e.g. word that appear in, for example, only in 3% in the documents of the corpus) are not going to be relevant for ou topic analysis, aren't they? \\n\",\n",
    "    \"\\n\",\n",
    "    \"Thinkg about this a little bit, and let this sink in.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Does it makes sense now? Awesome!\\n\",\n",
    "    \"\\n\",\n",
    "    \"So, we now need a way to understand which words are which.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The way we are going to do this is:\\n\",\n",
    "    \"1. Creating a vocabulary - a set of all the terms in the corpus\\n\",\n",
    "    \"2. Score all the terms with a metric that help us understand the incident of a given term in a corpus.\\n\",\n",
    "    \"    - This score will put the rare events in one extreme, and the constant events the another extreme (e.g. rare terms having the higher values, and constant terms having the lower values).\\n\",\n",
    "    \"    - The above line yields that, for topic analysis, we want the words that are in between of those two\\n\",\n",
    "    \"3. Based on these metrics, let's filter the terms and stay only with those relevant for topic analysis \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 2.4 Creating a vocabulary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Given that this corpus has a lot of document, I am going to sample the corpus and will only use 100 documents\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 24,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"My vocabolary size is 7642\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"tokenized_corpus_sampled = tokenised_corpus[:100]\\n\",\n",
    "    \"\\n\",\n",
    "    \"vocab = set()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for doc in tokenized_corpus_sampled:\\n\",\n",
    "    \"    vocab = vocab.union(set(doc))  \\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"My vocabolary size is {len(vocab)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 2.5 Scoring extreme terms\\n\",\n",
    "    \"\\n\",\n",
    "    \"The metric we are going to use to score the terms and therefore to get the extreme event is going to be the *IDF* metric.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### The IDF Metric\\n\",\n",
    "    \"\\n\",\n",
    "    \"Suppose  for  example  that  in  our  corpus  the  word  ‘rice’  appears  in  every  document.   \\n\",\n",
    "    \"Suppose that you are the librarian keeping this corpus,  and somebody comes searching for a subset of documents in a given topic from your corpus.  \\n\",\n",
    "    \"Imagine this library visitor tells you ‘rice’.  You go into the box to fetch all the documents that contain that word.  Clearly you will come backwith the entire box because every document in it contains that term.  \\n\",\n",
    "    \"Was ‘rice’ a helpful term to support the library visitor’s needs?  Not really.  In fact not helpful at all. \\n\",\n",
    "    \"\\n\",\n",
    "    \"This is where Inverse Document Frequency or IDF comes in handy.  This number will represent the importance of a term in a given corpus, calculated using the following formula:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$IDF = log(\\\\frac{N}{df_t})$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Where:\\n\",\n",
    "    \"- N is the number of document in the corpus\\n\",\n",
    "    \"- $df_t$ is the number of documents where the term $t$ is in\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 26,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def idf(term, corpus):\\n\",\n",
    "    \"    cnt =  sum([1 if term in doc else 0 for doc in corpus])\\n\",\n",
    "    \"    return math.log10( len(corpus) / cnt )\\n\",\n",
    "    \"\\n\",\n",
    "    \"idfvocab = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"for term in vocab:\\n\",\n",
    "    \"    term_idf = idf(term, tokenized_corpus_sampled)\\n\",\n",
    "    \"    idfvocab[term] = term_idf\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 2.6 Let's filter out those extreme events\\n\",\n",
    "    \"\\n\",\n",
    "    \"So, we now have our terms with a given metric scored. \\n\",\n",
    "    \"For us to filter the extreme events, there are several ways for doing so. \\n\",\n",
    "    \"\\n\",\n",
    "    \"We can think that we want to remove, for example, the upper 25% of the tokens and the lower 25% of the tokens.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We can also say that we want, at most only 200 tokens that are somewhere in the middle of the distribution to pass this filter.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The following example will be with the latter mindset.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 27,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Min is 0.0 and max is 2.0\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"idfvocab_it = [(el[0],el[1]) for el in idfvocab.items()]\\n\",\n",
    "    \"\\n\",\n",
    "    \"aux = np.array( idfvocab_it )\\n\",\n",
    "    \"low = float( min( aux[:,1] ) )\\n\",\n",
    "    \"high = float( max( aux[:,1] ) )\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Min is {low} and max is {high}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 29,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"150\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 29,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"def keep_terms( lower, upper, threshold, step, idf_vocabulary ):\\n\",\n",
    "    \"    low = lower\\n\",\n",
    "    \"    up = upper\\n\",\n",
    "    \"    candidates = idf_vocabulary\\n\",\n",
    "    \"    while len(candidates) > threshold:\\n\",\n",
    "    \"        #print(f\\\"current vocabolary size is {len(candidates)}\\\")\\n\",\n",
    "    \"        low = low + step\\n\",\n",
    "    \"        up = up - step\\n\",\n",
    "    \"        candidates = [  term for term in idf_vocabulary if term[1] >= low and term[1] <= up  ]\\n\",\n",
    "    \"    return candidates\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"cnd = keep_terms(low, high, 200, 0.005, idfvocab_it)\\n\",\n",
    "    \"len(cnd)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"By the end of this process, we now have our Bag of Words to continue this analysis! Notice that every document is going to be encoded with this bag of words in a vector space - we now have a unified (single) corpus representation!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Construction of the TF-IDF Matrix\\n\",\n",
    "    \"\\n\",\n",
    "    \"This step is the core of the STA life cycle. What  is  interesting  here  is  that,  while  starting with a corpus made of disjoint elements (a collection of documents),  we end up with a single corpus representation.  \\n\",\n",
    "    \"\\n\",\n",
    "    \"This single representation unifies the information we have about the comprised documents.  Therefore we can use this single data structure to reason about the entire corpus!  \\n\",\n",
    "    \"For this goal, working with an universal dictionary of terms is essential.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### The TF Metric\\n\",\n",
    "    \"\\n\",\n",
    "    \"Term Frequency or simply TF is a numeric quantity used to express the importance of a term inside a document.  \\n\",\n",
    "    \"In its raw form, it is simply the count of times a term appears in a document.\\n\",\n",
    "    \"\\n\",\n",
    "    \"However, here we compute TF as a proportion, by dividing this count by the total number of tokens  in  the  (stemmed)  document.   \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 30,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"vc = np.array(cnd) #a matrix, with column 0 being terms and column 1 being idf\\n\",\n",
    "    \"vc_terms = vc[:,0] \\n\",\n",
    "    \"\\n\",\n",
    "    \"def normTFx(term,doc):\\n\",\n",
    "    \"    return doc.count(term)/len(doc)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### The TF-IDF Metric\\n\",\n",
    "    \"\\n\",\n",
    "    \"The final quantity that we will use to measure the importance of a term inside a document that belongs to a corpus os the standard TF.IDF measure which is simply:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$TF.IDF^d_t = TF^d_t * IDF_t$\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here $t$ refers as always to the term, and $d$ to a specific document. \\n\",\n",
    "    \"\\n\",\n",
    "    \"What is the effect of multiplying the original normalised TF by the IDF? The IDF acts as a modulator.  If the TF is high but the term is everywhere in the corpus, the IDF will be low, so the TF is brought down.  If a TF is medium, but the IDF is high then its importance is modulated upwards.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 33,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def tfidfmat(corpus, tl) :\\n\",\n",
    "    \"    mat =[]\\n\",\n",
    "    \"    for i in tl :\\n\",\n",
    "    \"        idft = idf(i,corpus)\\n\",\n",
    "    \"        row = []\\n\",\n",
    "    \"        for d in corpus :\\n\",\n",
    "    \"            tft = normTFx(i,d)\\n\",\n",
    "    \"            tf_idf_term_document = tft*idft\\n\",\n",
    "    \"            row.append(tf_idf_term_document)\\n\",\n",
    "    \"        mat.append(row)\\n\",\n",
    "    \"    return mat    \\n\",\n",
    "    \"            \\n\",\n",
    "    \"    \\n\",\n",
    "    \"\\n\",\n",
    "    \"tfidf_matrix = tfidfmat(tokenized_corpus_sampled, vc_terms) \\n\",\n",
    "    \"tfidf_matrix_np = np.array(tfidf_matrix)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 34,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"(150, 100)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 34,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"tfidf_matrix_np.shape\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 35,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"The document with index 0 contains 833 words\\n\",\n",
    "      \"The term with index 0 is `parent`\\n\",\n",
    "      \"The importance of the term `parent` in the document with idx = 0 is 0.0\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"print(f\\\"The document with index 0 contains {len(tokenized_corpus_sampled[0])} words\\\")\\n\",\n",
    "    \"print(f\\\"The term with index 0 is `{vc_terms[0]}`\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"The importance of the term `{vc_terms[0]}` in the document with idx = 0 is {tfidf_matrix_np[0,0]}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Now  we  have  all  we  need  to  construct  our  single representation  of  the  corpus  as  a  matrix.   \\n\",\n",
    "    \"\\n\",\n",
    "    \"This  matrix has  rows representing  the  terms  of the  universal  dictionary  for  the  corpus,  and  columns  representing  the  contained  documents. Therefore a given cell $S_{t,d}$ of the matrix will contain the corresponding $TF.IDF^d_t$.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Factorizing the Matrix\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 36,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.decomposition import NMF\\n\",\n",
    "    \"model = NMF(n_components=5, init='random', random_state=0)\\n\",\n",
    "    \"W = model.fit_transform(tfidf_matrix_np) # loadings\\n\",\n",
    "    \"H = model.components_ #scores\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 37,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"(150, 5)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 37,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"W.shape\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 38,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"(5, 100)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 38,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"H.shape\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 46,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def get_top_N_terms(matrix_slice, N):\\n\",\n",
    "    \"    return matrix_slice.argsort()[-N:]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 55,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def get_terms_from_slice(loadings_matrix, idx, topN, bag_of_words, orientation=\\\"col\\\"):\\n\",\n",
    "    \"    '''\\n\",\n",
    "    \"        the parameter `orientation` can either be \\\"col\\\" or \\\"row\\\", so we can process a loadings matrix being it transposed or not\\n\",\n",
    "    \"    '''\\n\",\n",
    "    \"    k = None\\n\",\n",
    "    \"    if orientation == \\\"col\\\":\\n\",\n",
    "    \"        k = loadings_matrix[:,idx]\\n\",\n",
    "    \"    elif orientation == \\\"row\\\":\\n\",\n",
    "    \"        k = loadings_matrix[idx,:]\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        raise Exception(\\\"Orientation not recognized\\\")\\n\",\n",
    "    \"    k_top5terms_idx = get_top_N_terms(k,topN)\\n\",\n",
    "    \"    return bag_of_words[k_top5terms_idx]\\n\",\n",
    "    \"    \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 60,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"The terms with more weight in the component 0 are: ['onlin' 'growth' 'video' 'experi' 'audienc' 'educ' 'publish']\\n\",\n",
    "      \"The terms with more weight in the component 1 are: ['threaten' 'citizen' 'senior' 'educ' 'health' 'crisi' 'senat']\\n\",\n",
    "      \"The terms with more weight in the component 2 are: ['investor' 'air' 'rate' 'educ' 'negoti' 'pay' 'attend']\\n\",\n",
    "      \"The terms with more weight in the component 3 are: ['goal' 'titl' 'franc' 'twitter' 'host' 'central' 'met']\\n\",\n",
    "      \"The terms with more weight in the component 4 are: ['citizen' 'review' 'health' 'refus' 'appeal' 'lawyer' 'polic']\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"for k in range(0,W.shape[1]):\\n\",\n",
    "    \"    # Get terms for the k-th characteristic / topic\\n\",\n",
    "    \"    print(f\\\"The terms with more weight in the component {k} are: {get_terms_from_slice(W, k, 7, vc_terms)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# here we are printing the top 7, but the this choise is arbitrary - we are going to analyze as much as we need to understand the topics\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.7.3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
